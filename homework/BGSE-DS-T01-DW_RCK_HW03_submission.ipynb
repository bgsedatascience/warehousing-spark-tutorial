{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the following code to create my cluster. I was not able to use n1-standard-4 because of CPU quota constraints, which may have affected the performance.\n",
    "\n",
    "```\n",
    "$CLUSTER=bgse-ds-hw03-cluster\n",
    "$PROJECT=bgsedatasciencehw\n",
    "$BUCKET=bgse_datascience_hw_bucket\n",
    "\n",
    "gcloud beta dataproc clusters create $CLUSTER \\\n",
    "--optional-components=ANACONDA,JUPYTER \\\n",
    "--image-version=1.3 \\\n",
    "--enable-component-gateway \\\n",
    "--bucket $BUCKET \\\n",
    "--project $PROJECT \\\n",
    "--zone europe-west1-b \\\n",
    "--region europe-west1 \\\n",
    "--worker-machine-type n1-standard-2 \\\n",
    "--num-workers=2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from itertools import permutations\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "from google.cloud import storage\n",
    "\n",
    "import json \n",
    "import datetime as dt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file_list(bucket_name = \"bgse-datawarehousing-random-tweets\"):\n",
    "    start = dt.datetime.now()\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    json_file_list = []\n",
    "    for idx, blob in enumerate(bucket.list_blobs(prefix=\"\")):\n",
    "        json_file_list.append(\"gs://\" + bucket_name + \"/\" + blob.name)\n",
    "    end = dt.datetime.now()\n",
    "    print(\"Create file list : [ {0} ] files found :: Elapsed time :: [ {1} ]\".format(idx+1, end - start))\n",
    "    return json_file_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_files(passed_list, stop=0):\n",
    "    start = dt.datetime.now()\n",
    "    if stop == 0:\n",
    "        stop = len(passed_list)\n",
    "    data = spark.read.json(passed_list[0:stop])\n",
    "    end = dt.datetime.now()\n",
    "    print(\"Load file list : [ {0} ] files :: Elapsed time :: [ {1} ]\".format(stop, end - start))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_value_list(data, backup=False):\n",
    "    start = dt.datetime.now()\n",
    "\n",
    "    # Taking only lists with more than one element\n",
    "    value_list = data.rdd \\\n",
    "        .filter(lambda l: ((l.entities is not None) and \\\n",
    "                                (len(l.entities.hashtags) > 1) )) \\\n",
    "        .map(lambda l: [i.text.lower() for i in l.entities.hashtags]) \\\n",
    "        .flatMap(lambda l: permutations(l, 2)) \\\n",
    "        .map(lambda l: (l, 1)) \\\n",
    "        .reduceByKey(lambda a, b: a + b) \\\n",
    "        .map(lambda l: (l[0][0], l[0][1], l[1])) \\\n",
    "        .collect()\n",
    "\n",
    "    end = dt.datetime.now()\n",
    "    \n",
    "    if backup:\n",
    "        with open('value_list.json', 'w') as outfile:\n",
    "            json.dump(value_list, outfile)\n",
    "            \n",
    "    print(\"Create value list :: Elapsed time :: [ {0} ]\".format(end - start))\n",
    "    return value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_column_list(value_list, backup=False):\n",
    "    start = dt.datetime.now()\n",
    "\n",
    "    unique = list(set([x[0] for x in value_list]))\n",
    "    sort_list = sorted(unique)\n",
    "    dict_column_names = { sort_list[i] : i for i in range(0, len(sort_list) ) }\n",
    "\n",
    "    if backup:\n",
    "        with open('column_names.json', 'w') as outfile:\n",
    "            json.dump(dict_column_names, outfile)\n",
    "            \n",
    "    end = dt.datetime.now()   \n",
    "    print(\"Create column list :: Elapsed time :: [ {0} ]\".format(end - start))\n",
    "    return dict_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse_matrix(dict_column_names, value_list):\n",
    "    start = dt.datetime.now()\n",
    "    \n",
    "    row = np.array([dict_column_names[i[0]] for i in value_list])\n",
    "    column = np.array([dict_column_names[i[1]] for i in value_list])\n",
    "    data = np.array([i[2] for i in value_list])\n",
    "    sp_mat = coo_matrix((data, (row,column)),\n",
    "                        shape=(len(dict_column_names),\n",
    "                               len(dict_column_names)))\n",
    "    \n",
    "    end = dt.datetime.now()   \n",
    "    print(\"Create sparse matrix :: Elapsed time :: [ {0} ]\".format(end - start))\n",
    "    return sp_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_from_backup(col_names = 'column_names.json', val_list = 'value_list.json'):\n",
    "    start = dt.datetime.now()\n",
    "    \n",
    "    with open(col_names, 'r') as infile:\n",
    "        dict_column_names = json.load(infile)\n",
    "    with open(val_list, 'r') as infile:\n",
    "        value_list = json.load(infile)\n",
    "    \n",
    "    end = dt.datetime.now()   \n",
    "    print(\"Reload files :: Elapsed time :: [ {0} ]\".format(end - start))\n",
    "    return value_list, dict_column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runScript(backup = True, files = 0, singleFile = False):\n",
    "    initial_start = dt.datetime.now()\n",
    "    #initialise_logger()\n",
    "    if singleFile:\n",
    "        json_file_list = [\"gs://bgse-datawarehousing-random-tweets/2019-02-26T00:00:30.657Z\"]\n",
    "        files = 1\n",
    "    else:\n",
    "        json_file_list = create_file_list()\n",
    "\n",
    "    if (files == 0):\n",
    "        files = len(json_file_list)\n",
    "        print(\"Files unset; running on entire set.\")\n",
    "    data = load_json_files(json_file_list, stop = files)\n",
    "    \n",
    "    #data.printSchema()\n",
    "    \n",
    "    value_list = create_value_list(data, backup)\n",
    "    dict_column_names = create_column_list(value_list, backup)\n",
    "\n",
    "    if backup:\n",
    "        value_list, dict_column_names = reload_from_backup()\n",
    "    create_sparse_matrix(dict_column_names, value_list)\n",
    "\n",
    "    print(\"Overall Script :: Elapsed time :: [ {0} ]\".format(dt.datetime.now() - initial_start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_files():\n",
    "    value_list, dict_column_names = reload_from_backup()\n",
    "    sp_mat = create_sparse_matrix(dict_column_names, value_list)\n",
    "    return sp_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This runs the script but for the full dataset it stops producing output in the middle. As a result, I backed up the values to .json files, which allow the sparse matrix to be rebuilt. I think that running it as a script might improve the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time :: 2019-10-28 13:39:45.690149\n",
      "Create file list : [ 6108 ] files found :: Elapsed time :: [ 0:00:00.688136 ]\n",
      "Files unset; running on entire set.\n",
      "Load file list : [ 6109 ] files :: Elapsed time :: [ 0:27:37.927885 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Start time :: \" + str(dt.datetime.now()))\n",
    "runScript()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reload files :: Elapsed time :: [ 0:00:05.221110 ]\n",
      "Create sparse matrix :: Elapsed time :: [ 0:00:04.153591 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<260019x260019 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 2561237 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_mat = load_from_files()\n",
    "sp_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz(\"sparseMatrix.npz\", sp_mat, compressed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then created a terminal through Jupyter and then ran the following:\n",
    "```\n",
    "cd /\n",
    "gsutil cp sparseMatrix.npz gs://bgse_datascience_hw_bucket\n",
    "```\n",
    "\n",
    "Then, on my local machine, go to your GitHub folder and run the following\n",
    "```\n",
    "cd $GITHUB/DataScience/Trimester01/DataWarehousingAndBusinessIntelligence/warehousing-spark-tutorial/homework\n",
    "gsutil cp gs://bgse_datascience_hw_bucket/sparseMatrix.npz .\n",
    "gsutil cp gs://bgse_datascience_hw_bucket/notebooks/jupyter .\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}