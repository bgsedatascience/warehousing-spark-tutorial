{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.7.14 |Anaconda, Inc.| (default, Dec  7 2017, 17:05:42) \\n[GCC 7.2.0]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print 'hello world!'\n",
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the Google Cloud client library\n",
    "#from google.cloud import storage\n",
    "from pyspark.sql import SparkSession\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "from scipy.sparse import coo_matrix, save_npz\n",
    "import numpy as np\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(filename='spark-assignment.log',level=logging.DEBUG)\n",
    "logging.warning('this is the beginning of the log file')\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"PySpark Intro\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "bucket_dir = 'gs://bgse-datawarehousing-random-tweets'\n",
    "file1_name = 'gs://bgse-datawarehousing-random-tweets/2019-02-26T00:00:30.657Z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Functions\n",
    "def rdd_transforms(files):\n",
    "\n",
    "    tweets = files.rdd.map(lambda x: x.entities).filter(lambda x: x is not None) \\\n",
    "        .map(lambda x: x.hashtags).filter(lambda x: x is not None) \\\n",
    "        .filter(lambda x: len(x) > 1).map(lambda x: [i[1] for i in x]) \\\n",
    "        .map(lambda x: [i.lower() for i in x]).cache()\n",
    "\n",
    "    pairs =  tweets.flatMap(lambda x: sorted(list(combinations(x,2)))).cache()\n",
    "    tags = tweets.flatMap(lambda x: [i for i in x]).distinct().cache()\n",
    "\n",
    "    counts = pairs.map(lambda x: [x,1]).reduceByKey(lambda a,b: a + b).cache()\n",
    "\n",
    "    dict_fromTag = tags.zipWithIndex().collectAsMap()\n",
    "    \n",
    "    return counts,dict_fromTag\n",
    "\n",
    "def output_sparse_vectors(counts,dict_fromTag):\n",
    "    myrows = []\n",
    "    mycols = []\n",
    "    mydata = []\n",
    "\n",
    "    myrows = counts.map(lambda x: dict_fromTag[x[0][0]]).collect() + \\\n",
    "        counts.map(lambda x: dict_fromTag[x[0][1]]).collect()\n",
    "    mycols = counts.map(lambda x: dict_fromTag[x[0][1]]).collect() + \\\n",
    "        counts.map(lambda x: dict_fromTag[x[0][0]]).collect()\n",
    "\n",
    "    mydata = counts.map(lambda x: x[1]).collect()\n",
    "    mydata = mydata + mydata\n",
    "    \n",
    "    return myrows, mycols, mydata\n",
    "\n",
    "def print_runningtime(start_time):\n",
    "    print('%s minutes elapsed.' % str((datetime.datetime.now()-start_time).total_seconds()/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-30 20:18:20.308611\n",
      "2.4507813 minutes elapsed.\n",
      "40.5972096 minutes elapsed.\n",
      "43.9376648833 minutes elapsed.\n",
      "43.9473593833 minutes elapsed.\n",
      "2019-10-30 21:02:17.150356\n",
      "44.0061819333 minutes elapsed.\n",
      "2019-10-30 21:02:20.679769\n"
     ]
    }
   ],
   "source": [
    "#Prod\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "print(start_time)\n",
    "logging.warning('Beginning run at %s' % (datetime.datetime.now()))\n",
    "\n",
    "files = spark.read.options(samplingRatio=0.01).json(bucket_dir)\n",
    "print_runningtime(start_time)\n",
    "logging.warning('Finished loading files at %s' % datetime.datetime.now())\n",
    "\n",
    "counts, dict_fromTag = rdd_transforms(files)\n",
    "print_runningtime(start_time)\n",
    "logging.warning('Finished rdd_transforms at %s' % datetime.datetime.now())\n",
    "\n",
    "myrows, mycols, mydata = output_sparse_vectors(counts, dict_fromTag)\n",
    "print_runningtime(start_time)\n",
    "logging.warning('Finished outputting sparse vectors at %s' % datetime.datetime.now())\n",
    "\n",
    "mat = coo_matrix((mydata,(myrows,mycols)),shape=(len(dict_fromTag),len(dict_fromTag)))\n",
    "print_runningtime(start_time)\n",
    "print(datetime.datetime.now())\n",
    "logging.warning('Finished creating matrix at %s' % datetime.datetime.now())\n",
    "\n",
    "save_npz(\"output_mat_final.npz\", mat, compressed=True)\n",
    "\n",
    "print_runningtime(start_time)\n",
    "print(datetime.datetime.now())\n",
    "logging.warning('Finished saving sparse matrix at %s' % datetime.datetime.now())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}