{"nbformat_minor": 2, "cells": [{"execution_count": 1, "cell_type": "code", "source": "from pyspark.sql import SparkSession\nimport pandas as pd\nimport scipy as sc\nfrom itertools import permutations\nfrom scipy.sparse import coo_matrix\nimport numpy as np\nfrom google.cloud import storage\nimport json\nimport scipy\nimport os", "outputs": [], "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "client = storage.Client()\nbucket = client.bucket('bgse-datawarehousing-random-tweets')\npaths = []\nfor blob in bucket.list_blobs(prefix=''):\n    paths += ['gs://bgse-datawarehousing-random-tweets/'+blob.name]\n", "outputs": [], "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "def read_data(path_list):\n    return spark.read.json(path_list)", "outputs": [], "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": "def get_value_list(data_):\n    a=data_.rdd \\\n        .filter(lambda l: l.entities is not None) \\\n        .filter(lambda x: len(x.entities.hashtags) > 1) \\\n        .map(lambda x: [i.text.lower() for i in x.entities.hashtags]) \\\n        .flatMap(lambda r: permutations(r,2)) \\\n        .map(lambda q: (q,1)) \\\n        .reduceByKey(lambda a,b:a+b ) \\\n        .collect()\n\n    with open('list_of_values.json','w') as outfile:\n        json.dump(a,outfile)\n    return a", "outputs": [], "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "def get_sorted_unique_values_dict(val_list):\n    sort_list = sorted(list(set([i[0][0] for i in val_list])))\n    dict_of_words = {sort_list[i] : i for i in range(0,len(sort_list))}\n    with open('col_names_dict.json','w') as outfile:\n        json.dump(dict_of_words,outfile)\n    return dict_of_words\n", "outputs": [], "metadata": {}}, {"execution_count": 6, "cell_type": "code", "source": "def create_sparse_matrix(val_list,dict_cols):\n    row  = np.array([dict_cols[i[0][0]] for i in val_list])\n    col  = np.array([dict_cols[i[0][1]] for i in val_list])\n    data = np.array([i[1] for i in val_list])\n    return coo_matrix((data, (row, col)), shape=(len(dict_cols), len(dict_cols)))", "outputs": [], "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": "def get_backup(col_names = 'col_names_dict.json',value_list = 'list_of_values.json'):\n    with open(col_names, 'r') as infile:\n        dict_col_names = json.load(infile)\n    with open(value_list,'r') as infile:\n        val_list = json.load(infile)\n    return val_list,dict_col_names", "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "#def main_program():\ndata = read_data(paths)\nprint('data loaded')\nvalue_list = get_value_list(data)\nprint('value_list created')\ndict_columns = get_sorted_unique_values_dict(value_list)\nprint('dict_columns created')\nvalue_list, dict_columns = get_backup()\nprint('Read from backup')\nsparse_mat = create_sparse_matrix(value_list,dict_columns)\nscipy.sparse.save_npz('sparse_matrix.npz', sparse_mat)\n#return sparse_mat", "outputs": [{"output_type": "stream", "name": "stdout", "text": "data loaded\nvalue_list created\ndict_columns created\nRead from backup\n"}], "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": "reloaded_matrix = scipy.sparse.load_npz('sparse_matrix.npz')", "outputs": [], "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": "reloaded_matrix\n", "outputs": [{"execution_count": 10, "output_type": "execute_result", "data": {"text/plain": "<260019x260019 sparse matrix of type '<type 'numpy.int64'>'\n\twith 2561237 stored elements in COOrdinate format>"}, "metadata": {}}], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark", "name": "pyspark", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.14", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}