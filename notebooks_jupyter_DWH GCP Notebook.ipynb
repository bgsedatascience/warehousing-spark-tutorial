{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## from itertools import permutations \n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "#hasht = spark.read.json('gs://bgse-datawarehousing-random-tweets/2019-02-26T00:00:30.657Z')\n",
    "\n",
    "client=storage.Client()\n",
    "bucket=client.bucket(\"bgse-datawarehousing-random-tweets\")\n",
    "json_list=[]\n",
    "for blob in bucket.list_blobs(prefix=\"\"):\n",
    "    json_list.append(\"gs://bgse-datawarehousing-random-tweets/\"+blob.name)  \n",
    "\n",
    "# Read in the first 5 files\n",
    "hasht= spark.read.json(json_list[0:6109])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#hasht.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hasht.rdd.filter(lambda r: r.entities.hashtags).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=hasht.rdd \\\n",
    "    .filter(lambda o: o.entities is not None) \\\n",
    "    .map(lambda o: o.entities) \\\n",
    "    .filter(lambda o: len(o.hashtags) > 1) \\\n",
    "    .map(lambda o:[i.text.lower() for i in o.hashtags]) \\\n",
    "    .flatMap(lambda o:permutations(o,2)) \\\n",
    "    .map(lambda l: (l,1)) \\\n",
    "    .reduceByKey(lambda a, b: a+b) \\\n",
    "    .map(lambda l:(l[0][0], l[0][1], l[1])) \\\n",
    "    .collect()\n",
    "    \n",
    "    #.map(lambda o: o.price_each * o.quantity_ordered)\\\n",
    "   # .reduce(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c=Counter(elem[0] for elem in x)\n",
    "len(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 9, 1, ..., 2, 1, 1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows=np.array([c[i[0]] for i in x])\n",
    "columns=np.array([c[i[1]] for i in x])\n",
    "data=np.array([i[2] for i in x])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "\n",
    "sparse_mat=scipy.sparse.coo_matrix((data,(rows, columns)), shape=(len(c),len(c)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz(\"sparseMatrix.npz\", sparse_mat, compressed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}